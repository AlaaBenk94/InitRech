#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass report
\begin_preamble
\usepackage[bottom]{footmisc}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{minitoc}
\usepackage{pgfplots}
\usepackage{lmodern}

\usepackage{titletoc}% http://ctan.org/pkg/titletoc
\titlecontents{chapter}% <section-type>
  [0pt]% <left>
  {\bfseries}% <above-code>
  {\chaptername\ \thecontentslabel:\quad}% <numbered-entry-format>
  {}% <numberless-entry-format>
  {\hfill\contentspage}% <filler-page-format>

\fancyhead[R,C]{}
\fancyhead[L]{\footnotesize\leftmark}
\renewcommand{\headrulewidth}{1pt}
\fancyfoot[L,C]{}
\fancyfoot[R]{\thepage}
\renewcommand{\footrulewidth}{0pt}

\oddsidemargin = 18pt
\topmargin = 0pt
\headheight = 22pt
\headsep = 25pt
\textheight = 609pt
\textwidth = 424pt
\marginparsep = 11pt
\marginparwidth = 54pt
\footskip = 50pt
\marginparpush = 54pt
\hoffset = 0pt
\voffset = 0pt
\end_preamble
\use_default_options true
\begin_modules
theorems-bytype
\end_modules
\maintain_unincluded_children false
\language french
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement h
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\headheight 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swiss
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 8 -1
\bullet 2 0 9 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Analyse et réalisation
\end_layout

\begin_layout Quotation
\align right

\emph on
\begin_inset Quotes cld
\end_inset

...si vous ne pouvez pas l'expliquer simplement, vous ne le comprenez pas assez
 bien.
\begin_inset Quotes crd
\end_inset

.
 A.
 Einstein.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
minitoc
\end_layout

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
Les deux premiers chapitres étaient une introduction au contexte du projet
 ainsi q'une présentation de toutes les notions importantes pour la compréhensio
n de l'approche.
\end_layout

\begin_layout Standard
Dans ce chapitre, nous présentrons les différentes étapes de l'approche
 suivie en expliquant ce que nous avons utilisé dans chacune de ces étapes.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{section}{Introduction}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Appoche globale
\end_layout

\begin_layout Standard
Notre objectif, comme mentionné précédemment, est de concevoir un système
 qui permet à la lampe de réagir en fonction des changements des expressions
 du sujet qui se trouve en face d'elle.
\end_layout

\begin_layout Standard
La réalisation de ce système nécessite de passer par plusieurs étapes indispensa
bles en commençant par la détection de visage et l'extraction des points
 de saillances.
 Ensuite nous allons passer à l'extraction des caractéristiques et l'encodage
 des solutions.
 Enfin, nous arrivons au regroupement (clustering) des différents changements
 et à la définition du comportement.
\end_layout

\begin_layout Standard
Pour simplifier la compréhension du fonctionnement de l'approche suivie,
 nous avons jugé utile de partitionner le travail en deux phases (figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Structure-globale-du"
plural "false"
caps "false"
noprefix "false"

\end_inset

) :
\end_layout

\begin_layout Itemize
La phase de préparation qui prend en entrée un flux d'image sous forme d'une
 vidéo (séquences d'images) pour produire en sortie un vecteur caractérisant
 les changements sur les différentes séquences.
\end_layout

\begin_layout Itemize
La deuxième phase, dite d'extraction des vecteurs caracteristiques des changemen
ts 
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "VCC"
description "Vecteurs Caracteristiques des Changements"
literal "false"

\end_inset

VCC, comporte deux étapes : l'extraction des caracterisitques, puis, le
 calcule des vecteurs caracteristiques des changements d'expressions.
\end_layout

\begin_layout Itemize
La derniere phase est la phase de décision qui consiste à regrouper les
 vecteurs caractéristiques en entrée afin de décider le comportement à exécuter
 (les commandes à envoyer à la lampe).
 Nous verrons ses phases en détail dans les sections qui suit.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/schema.svg
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Structure-globale-du"

\end_inset

Structure globale du systèùme
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Phase de préparation
\end_layout

\begin_layout Standard
Dans cette section nous entamons la phase de préparation, cette phase a
 pour but d'extraire un vecteur de points de saillances pour les faire passer
 à la phase suivante.
\end_layout

\begin_layout Subsection
Prétraitement
\end_layout

\begin_layout Standard
Le prétraitement est l'étape avant-première (ou l'étape zéro), dans laquelle
 nous récupérons les images RGB de flux vidéo.
 Ensuite, nous les convertirons en niveaux de gris (grayscale) car nous
 n’avons pas besoin des couleurs dans nos prochains traitements (détection
 du visage, extraction des points de saillances…).
 Ces images seront ainsi redimensionnées pour diminuer le temps des traitements.
\end_layout

\begin_layout Standard
Afin d’obtenir de bons résultats dans les prochaines étapes, nous avons
 jugé utile d’appliquer une égalisation d’histogramme en niveaux de gris.
 Cette dernière permet de mieux répartir les intensités des images à faible
 contraste pour obtenir des images de meilleure qualité.
 Un résumé de l’étape de prétraitement est illustré par la figure suivante.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/preprocessing.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Prétraitement
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Détection du visage
\end_layout

\begin_layout Standard
Comme illustré par la figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Détection-de-visage"
plural "false"
caps "false"
noprefix "false"

\end_inset

 ci-dessous, l'étape de détection du visage s’agit de retrouver la zone
 du visage (la région d'interet ROI) dans une image pour qu’on puisse y
 effectuer des traitements par la suite.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/detection.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Détection-de-visage"

\end_inset

Détection de visage
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
La méthode utilisée pour cette étape est l’histogramme de gradient orienté
 (HOG) décrit dans (section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Histogrammes-de-gradients"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Nous avons utiliser son implémentation dans la bibliothèque dlib pour extraire
 la région d'interet des images de flux video.
\end_layout

\begin_layout Subsection
Éxtraction des points de saillances
\end_layout

\begin_layout Standard
Une fois l’étape de détection de visage est terminée, l’étape d’extraction
 des points de saillances prend place.
 Cette étape utilise la région (ROI) extraite de l’étape précédente pour
 déterminer 68 points de saillances.
 Nous allons utiliser un algorithme dite « algorithme d’estimations des
 points de saillances » qui est basé sur une approche inventée par Vahid
 Kazemi et Josephine Sullivan en 2014 
\begin_inset CommandInset citation
LatexCommand cite
key "kazemi2014one"
literal "false"

\end_inset

 basé sur des arbres de regressions (un type d'arbre de décision).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/landmarks.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Éxtraction des points de saillances
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
L'idée de base est de définir 68 points spécifiques (appelés repères) qui
 existent sur chaque visage - le haut du menton, le bord extérieur de chaque
 œil, le bord intérieur de chaque sourcil, etc.
 Ensuite, utiliser un algorithme d'apprentissage automatique pour avoir
 un model capable de trouver ces 68 points sur n'importe quel visage.
\end_layout

\begin_layout Section
Phase d'extraction des VCC
\end_layout

\begin_layout Standard
Cette phase permet en premier, de définir des caracteristique d'expressions
 ,choisis explicitement, a partir des vecteurs de points de saillances.
 Ensuite, utiliser ces caractéristiques pour calculer les vecteurs caractéristiq
ues des changements d'expressions qui seront utilisé pour le clustering
 dans la phase qui suit.
\end_layout

\begin_layout Subsection
Éxtraction et encodagedes caractéristique
\end_layout

\begin_layout Standard
Effectuer un apprentissage sur tous les 68 points extraits est, d'une part,
 trés couteux en terme du temps et d'espace ainsi, il peuvent générer de
 bruit qui affecte la qualité des résultat obtenus.
 Par conséquent, nous allons extraire, à partir de ces points, des caracteristiq
ues qui nous permettent de destinguer les differentes expressions du visage.
 Nous avons consideré six caracteristiques (à savoir : distance des sourcils,
 ouverture des yeux, position du visage, ouverture de la bouche et la rotation
 du visage), ils sont illustrés sur la figure ci-dessous.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/features.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Éxtraction des caractéristiques
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Dans notre experience, la lampe rebotisé est mis devant differents personnes
 voire les mêmes perssonnes peuvent se retrouver à des distances differents
 de la lampe.
 Cela engendre une diversité de valeurs des caracteristiques définis ci-dessus
 pour chaque personne et aussi pour la meme personne avec des differents
 distances.
 De ce fait, il est nécéssaire de normaliser ce vecteur des caracteristique
 pour que la diversité des personnes (ou leurs distances de la camera) n'affecte
 pas les résultat retournés.
 Nous avons donc normaliser notre vecteur caracteristique, en prenant le
 rapports des caracteristiques sur des distance fixe dans le visage, par
 exemple le rapport entre la largeur et la longueur des yeux represente
 le proportion d'ouverture des yeux.
 En conséquence, la valeur de chacune des caracteristique est compris entre
 0 et 1.
\end_layout

\begin_layout Subsection
Vecteurs caractéristiques des changements d'expressions
\end_layout

\begin_layout Standard
Jusqu'à la, nous avons pu extraire les points des saillances et en définir
 des caracteristique avec.
 Donc on peut passer ces vecteurs pour la prochaine phase pour le clustering
 des expressions.
 Cependant, nous nous intéressons du 
\emph on
chagement des expressions
\emph default
 et non plus des expressions uniques.
 Pour cela, il faut traiter plusieurs images (séquence d'images) pour qu'on
 puisse calculer le vecteur caractérisant les changements d'expressions
 sur cette séquence.
\end_layout

\begin_layout Standard
Une approche simple pour le faire, est de prendre les images du flux video
 deux à deux.
 Puis, on calcule le changement entre ces deux images.
 Soientt deux images i et j, extraites de flux video dans deux instant diffèrent
s tel que 
\begin_inset Formula $temps(i)<temps(j)$
\end_inset

 .
 D'abord, les étapes de détéction du visage, d'éxtraction des points de
 saillances et d'extraction des caracteristiques sont effectués sur ces
 deux images pour avoir les vecteurs caracteristiques 
\begin_inset Formula $Vi$
\end_inset

 (respectivement 
\begin_inset Formula $Vj$
\end_inset

) des images 
\begin_inset Formula $i$
\end_inset

 (respectivement 
\begin_inset Formula $j$
\end_inset

).
 Ensuite, nous calculons le 
\emph on
vecteur caracteristique des changments 
\emph default
en appliquant une simple soustraction 
\begin_inset Formula $Vj-Vi$
\end_inset

 sauf pour la position, nous calculons la distance euclidienne.
 Nous obtenons finalement un vecteur caracterisant les chagments entre entre
 l'image 
\begin_inset Formula $i$
\end_inset

 et l'image 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/VCC.svg
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Calcul des vecteurs caractèristiques des changements
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Phase de décision
\end_layout

\begin_layout Standard
La dernière phase de l'approche, est celle de décision, dans laquelle nous
 prenons les vecteurs caractéristiques issues de la phase antérieure, et
 on y applique des algorithmes d’apprentissage pour avoir en sortie le comportem
ent approprié.
\end_layout

\begin_layout Standard
Nous pouvons formuler cela sous forme d'un problème d’apprentissage non
 supervisé discret où les des données sont les vecteurs caractéristiques
 des changements d'expresions et les clusters, ou encore classes ou groupes,
 représente les comportements.
 En effet, nous devons entrainer un model 
\begin_inset Formula $f$
\end_inset

 (une fonction de 
\begin_inset Formula $X$
\end_inset

 dans 
\begin_inset Formula $Y$
\end_inset

) qui permet de retourner pour chaque entrée 
\begin_inset Formula $x\in X$
\end_inset

 le comportement approprié 
\begin_inset Formula $y\in Y$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f:\ X\rightarrow Y
\]

\end_inset


\end_layout

\begin_layout Standard
\noindent
Avec 
\begin_inset Formula $X$
\end_inset

 ensemble des vecteurs caractéristiques des changements et 
\begin_inset Formula $Y$
\end_inset

 l’ensemble des comportements.
\end_layout

\begin_layout Standard
De plus, l’algorithme doit être capable d’apprendre en temps réel en raffinant
 le model à chaque nouvelle donnée d’entrée.
 Ce qui rend la tache plus complexe.
 Pour ce faire, nous avons décider d’employer une variante de la carte auto-orga
nisatrice.
\end_layout

\begin_layout Subsection
Carte auto-organisatrice dynamique
\end_layout

\begin_layout Standard
Nous avons vue en chapitre deux, la carte organisatrice de Kohonen et son
 fonctionnent.
 Pour notre problème, nous utilisons une des variantes des cartes auto-organisat
rices bi-dimensionnelle dite la carte auto-organisatrice dynamique (en anglais
 Dynamic Self Organizing Map - DSOM) proposé par Nicolas P.
 Rougier et Yann Boniface dans 
\begin_inset CommandInset citation
LatexCommand cite
key "rougier2011dynamic"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Au contraire de l'algorithme original de la carte organisatrice qui est
 dépendant du temps (taux d'apprentissage et voisinage), celui de DSOM est
 invariant dans le temps.
 Cela permet un apprentissage en ligne et continu sur les distributions
 de données statiques et dynamiques.
 De plus, la densité obtenue par cette variante n'est pas directement proportion
nelle à la densité de la distribution 
\begin_inset CommandInset citation
LatexCommand cite
key "rougier2011dynamic"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
La figure ci-dessus montrent la structure de la carte auto-organisatrice
 choisie pour concrétiser ce problème nous pouvons observer que les entrées
 et représentent nos vecteurs caractéristiques des changements et la sortie
 est une matrice de neurones dans chacun représente un comportement bien
 défini.
 Le neurone est représenté par un vecteur des poids de taille des entrées.
 Ces poids seront modifiés par apprentissage tout au long d'exécution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/DSOM.svg
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Structure de la DSOM utilisée
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Au départ, nous initialisons le poids des neurones aléatoirement.
 Ensuite pour chaque vecteur caractéristique des changements en entrée,
 nous récupérons d'abord les comportements appropriés à ce dernier en prenant
 le neurone qui a la plus petite distance (le meilleur match BMU).
 Puis, nous utilisons ce vecteur d'entrée et le BMU pour ajuster les poids
 des neurones.
 Cela correspond à l'entraînement de la DSOM.
 En effet, les poids de BMU et son voisinage, determiné par la fonction
 gaussienne, seront modifiés par la formule suivante 
\begin_inset CommandInset citation
LatexCommand cite
key "rougier2011dynamic"
literal "false"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Delta w_{i}=\varepsilon\parallel v-w_{i}\parallel h_{η}(i,s,v)(v-w_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
Avec :
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status collapsed

\begin_layout Plain Layout
-
\end_layout

\end_inset


\begin_inset Formula $\Delta w_{i}$
\end_inset

 : le nouveau poids et 
\begin_inset Formula $w_{i}$
\end_inset

 l'ancien poids
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status collapsed

\begin_layout Plain Layout
-
\end_layout

\end_inset


\begin_inset Formula $v$
\end_inset

 : le vecteur d'entrée
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status collapsed

\begin_layout Plain Layout
-
\end_layout

\end_inset


\begin_inset Formula $\varepsilon$
\end_inset

 : est la constante du taux d'apprentissage (learning rate)
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status collapsed

\begin_layout Plain Layout
-
\end_layout

\end_inset


\begin_inset Formula $h_{η}(i,s,v)$
\end_inset

 : la fonction du voisinage où
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
•
\end_layout

\end_inset


\begin_inset Formula $i$
\end_inset

 l'indice du neurone
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
•
\end_layout

\end_inset


\begin_inset Formula $s$
\end_inset

 représente le meilleure correspondant (BMU)
\end_layout

\begin_layout Itemize
\begin_inset Argument item:1
status open

\begin_layout Plain Layout
•
\end_layout

\end_inset


\begin_inset Formula $η$
\end_inset

 l'élasticité (un seuil pour considéré qu'un neurone et suffisament proche
 pour representer les données et donc les poids ne seront pas modifiés)
\end_layout

\end_deeper
\begin_layout Standard
Dans le cas où le neurone est suffisamment proche de la nouvelle entrée
 (distance < seuil), nous ne modifierons pas les poids car ce neurone est
 considéré comme représentant optimale du groupe dont cette donnée appartient.
 Nous répétons ces étapes tous le temps de l’exécution.
\end_layout

\begin_layout Section*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{section}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Nous avons présenté, dans ce chapitre, notre solution à la problématique
 posée au début de rapport.
 Nous avons obtenu de bons résultats.
 Cependant, certaines méthodes, utilisées dans les différentes étapes de
 la solution, présente des capacités assez limitées.
 Ces méthodes peuvent être subi à des amélioration et servir à améliorer
 les résultats.
\end_layout

\end_body
\end_document
